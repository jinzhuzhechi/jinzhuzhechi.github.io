<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Domain Generalization: A Survey</title>
    <link href="/posts/24868/"/>
    <url>/posts/24868/</url>
    
    <content type="html"><![CDATA[<h1>1.Introduction</h1><p><em>Answers to all these questions depend on how well the machine learning models can deal with one common problem, namely the domain shift problem. Such a problem refers to the distribution shift between a set of training (source) data and a set of test (target) data</em></p><p>未知数据域直接能够使模型工作的问题domain shift problem，主要是原数据和目标数据的分布不同</p><p><em>Most statistical learning algorithms strongly rely on an over-simplified assumption, that is, the source and target data are independent and identically distributed (i.i.d.), while ignoring out-of-distribution (OOD) scenarios commonly encountered in practice.</em></p><p>大多数数据的学习给予一个过度简化的假设：源数据和目标数据拥有独立且相同的分布（iid），忽略了超出分布的场景(OOD) 需要在实践中被考虑</p><p><em>A straightforward solution to bypass the OOD data issue is to collect some data from the target domain to adapt a source-domain-trained model</em></p><p>有一个简单的解决方案是从目标域中获取一些数据，来修正原始数据域上训练出的模型DA</p><p><em>DA relies on a strong assumption that target data is accessible for model adaptation, which does not always hold in practice</em></p><p>DA基于一个强假设，那就是目标域的数据是可获取用于模型修正的，但这在实践中不总成立</p><p><em>The goal in DG is to learn a model using data from a single or multiple related but distinct source domains in such a way that the model can generalize well to any OOD target domain.</em></p><p>DG的目标是学习一个从单一数据域或多个相关数据域能够很好的拓展到任意OOD的模型</p><h1>2.Background</h1><h2 id="2-1问题的提出">2.1问题的提出</h2><p>为了医学上的模型迁移到新的病人身上</p><p><em>They performed a thorough investigation into the cross-dataset generalization performance of object recognition models using six popular benchmark datasets. Their findings suggested that dataset biases, which are difficult to avoid, can lead to poor generalization performance.</em></p><p>在六个流行的数据集做了一组实验，发现数据集的差异无法避免，会导致很差的表现</p><h2 id="2-2问题的定义">2.2问题的定义</h2><p><em>Let X be the input (feature) space and Y the target (label) space, a domain is defined as a joint distribution PXY on X × Y.</em></p><p>x为输入空间（特征），y为目标空间（标签），而一个域被定义为分布x与y的joint distribution</p><p><em>For a specific domain PXY , we refer to PX as the marginal distribution on X, PY |X the posterior distribution of Y given X, and PX|Y the class-conditional distribution of X given Y .</em></p><p>对于一个特定的域PXY，我们把px称为x的边缘分布，PY |X 称为给定x的y的后验分布， PX|Y称为给定y的x的类条件分布</p><p><em>In the context of DG, we have access to K similar but distinct source domains S = {Sk = {(x(k), y(k))}}Kk=1, each associated with a joint distribution P (k) XY . Note that P (k) XY 6= P (k′) XY with k 6= k′ and k, k′ ∈ {1, …, K}.</em></p><p>在DG的语境中，我们对K个相似但不同的原始数据域$\mathcal{S} = \left{ S_{k} = \left{ \left( x<sup>{(k)},y</sup>{(k)} \right) \right} \right}_{k = 1}^{K}$每一个都对应一个联合概率分布。而不同数据域之间的分布都是不一样的</p><p><em>The goal of DG is to learn a predictive model f : X → Y using only source domain data such that the prediction error on an unseen target domain T = {xT } is minimized</em></p><p>DG的目标就是学习一个预测模型f : X → Y仅使用源数据域，是的再未知域上预测的误差减到最小</p><p>DG分两种</p><h3 id="Multi-Source-DG">Multi-Source DG</h3><p>方法大多是专用于多源的，默认K&gt;1</p><p><em>using multiple domains allows a model to discover stable patterns<br>across source domains, which generalize better to unseen domains</em></p><p>使用多数据源让模型在多个数据域能够发现稳定的模式，使模型能更好的拓展到未见的领域</p><h3 id="Single-Source-DG">Single-Source DG**</h3><p><em>Essentially, single-source DG methods do not require domain labels for<br>learning and thus they are applicable to multisource scenarios as well.</em></p><p>根本上，单源DG方法不需要标签来学习，所以他们对于多源也同样适用</p><h2 id="2-3Datasets-and-Applications">2.3Datasets and Applications</h2><ul class="lvl-0"><li class="lvl-2"><p>Handwritten Digit Recognition</p></li><li class="lvl-2"><p>Object Recognition目标检测</p></li></ul><p>In VLCS [56] and Office-31 [12], the domain shift is mainly caused<br>by changes in environments or viewpoints.源于环境和视角的转变</p><p>Image style changes have also been commonly studied, such as PACS [37]<br>(see Fig. 1©), OfficeHome [59], DomainNet [60], and<br>ImageNet-Sketch [51]</p><ul class="lvl-0"><li class="lvl-2"><p>Action Recognition动作识别</p></li><li class="lvl-2"><p>Semantic Segmentation语义分割</p></li><li class="lvl-2"><p>Person Re-Identification (Re-ID)行人重识别（比如监控跨摄像头追踪人）</p></li><li class="lvl-2"><p>Face Recognition</p></li><li class="lvl-2"><p>Face Anti-Spoofing人脸活体检测</p></li><li class="lvl-2"><p>Speech Recognition</p></li><li class="lvl-2"><p>Sentiment Classification</p></li><li class="lvl-2"><p>The WILDS Benchmark</p></li><li class="lvl-2"><p>Medical Imaging</p></li><li class="lvl-2"><p>Reinforcement Learning (RL)</p></li></ul><h2 id="2-4Evaluation">2.4Evaluation</h2><p><em>Evaluation of DG algorithms often follows the leave-one-domain-out rule [37]: given a dataset containing at least two distinct domains, one or multiple of them are used as source domain(s) for model training while the rest are treated as target domain(s);a model learned from the source domain(s) is directly tested in the target domain(s) without any form of adaptation</em></p><p>在一个或多个数据集上训练得模型，不加修改的使用在一个数据集上测试</p><h3 id="Evaluation-Metrics">Evaluation Metrics</h3><p>average and worst-case performance</p><h3 id="Model-Selection">Model Selection</h3><p>Trainingdomain validation, which holds out a subset of training data for<br>model selection;</p><p>Leave-one-domain-out validation, which keeps one source domain for model<br>selection;</p><p>Test-domain validation (oracle), which performs model selection using a<br>random subset of test domain data.</p><p><em>Another important lesson from [128] is that specially designed DG<br>methods often perform similarly with the plain model (known as Empirical<br>Risk Minimization) when using larger neural networks and an extensive<br>search of hyperparameters. Therefore, it is suggested that future<br>evaluation should cover different neural network architectures and<br>ensure comparison is made using the same model selection criterion.</em></p><p>特别设计的DG可能表现和一个简单的模型使用大一点的网络效果一样。所以在评估的时候最好和不同的神经网络一起比较，并使用相同的比较模型</p><h2 id="2-5-Related-Topics">2.5 Related Topics</h2><ul class="lvl-0"><li class="lvl-2"><p>Supervised Learning</p></li><li class="lvl-2"><p>Multi-Task Learning</p></li></ul><p><em>Intuitively, MTL benefits from the effect of regularization brought by<br>parameter sharing [129], which may in part explain why the MTL<br>paradigm works for DG.</em></p><p>特征的共享可能MTL在DG中有用</p><p>Transfer Learning (TL)</p><p><em>a couple of recent DG works [141], [142] have researched how to<br>preserve the transferable features learned via large-scale pre-training<br>when learning new knowledge from source synthetic data for<br>synthetic-to-real applications</em></p><p>几个DG网络研究如何在学习新知识时保留大型网络中可迁移的特征</p><p>Zero-Shot Learning</p><p>两者不同，ZSL主要是标签域在变动，而DG主要是数据域的变化</p><p>Domain Adaptation (DA)</p><p>Test-Time Training</p><p>blurs the boundary between DA and DG</p><p>均假设目标域不可见，但是会在运行测试集的时候对网络进行微调（在线），使用的数据集和DA高度重叠，但是要求算力较高，调整的时间较短</p><h1>3.METHODOLOGIES: A SURVEY</h1><h2 id="3-1Domain-Alignment">3.1Domain Alignment</h2><p><em>where the central idea is to minimize the difference among source<br>domains for learning domain-invariant representations (see Fig. 2).</em></p><p>中心思想是使用来学习的不同源域之间的差异最小化以获得跨领域不变的表示方法</p><p><em>The motivation is straightforward: features that are invariant to the<br>source domain shift should also be robust to any unseen target domain<br>shift</em></p><p>这么做的动机很简单，在源域变换中不变的特征应该足够鲁棒迁移到不可见的目标域中</p><p>PS：这个方法训练的时候需要标签</p><p><em>To measure the distance between distributions and thereby achieve<br>alignment, there are a wide variety of statistical distance metrics for<br>us to borrow, such as the simple `2 distance, f -divergences, or the<br>more sophisticated Wasserstein distance [217].</em></p><p>为了衡量两个分布之间的差距以达成适配，有许多统计学距离尺度，比如l2，f<br>-divergences，sophisticated Wasserstein distance</p><h3 id="3-1-1-What-to-Align">3.1.1 What to Align</h3><p>域是一个联合分布，我们把它表示成<br>$$<br>\mathcal{S}=\left{S_k=\left{\left(x^{(k)}, y<sup>{(k)}\right)\right}\right}_{k=1}</sup>K<br>$$</p><p><em>A common assumption in DG is that distribution shift only occurs in the<br>marginal P (X) while the posterior P (Y |X) remains relatively stable</em></p><p>一个通常的假设是在DG中分布变化只发生在x的边缘分布，而后验分布P (Y<br>|X)通常很稳定</p><p>（个人理解：就是说给定x后y是唯一的，但是x中有很多无关这个关系的信息，我们需要去Align出有用的消息）</p><p>当然还有一些换假设的，认为y本身不变，y是x原因…感觉crowd<br>c用不上也没大看懂原理</p><h3 id="3-1-2-How-to-Align">3.1.2 How to Align</h3><p>Minimizing Moments</p><p>缩小moment（a quantitative measure of the shape of a set of<br>points.），简单来说就是学习一个映射函数来减小分布之间均值和方差的差</p><p>Minimizing Contrastive Loss</p><p>给定一组锚，找到办法标定出与锚相同的数据和不同的数据，学习使差距减小和变大</p><p>Minimizing the KL Divergence 学习出一个方法使所有的域都能够服从高斯分布</p><p>Minimizing Maximum Mean Discrepancy (MMD) 先把数据映射到reproducing<br>kernel Hilbert space再得到距离</p><p>Domain-Adversarial Learning 使用对抗网络来减小差距</p><p>Multi-Task Learning</p><p>has also been explored for domain alignment [53], [206]. Different<br>from directly minimizing</p><h2 id="3-2-Meta-Learning（没看懂）">3.2 Meta-Learning（没看懂）</h2><p><strong>Episodes Construction</strong></p><p><strong>Meta-Representation</strong></p><h2 id="3-3-Data-Augmentation">3.3 Data Augmentation</h2><p>The basic idea in data augmentation is to augment the original (x, y)<br>pairs with new (A(x), y) pairs where A(·) denotes a transformation,<br>which is typically labelpreserving.</p><p>（这个感觉上对风格不同的照片会有帮助，或许可以尝试把天气什么的考虑进去做个增强？）</p><ul class="lvl-0"><li class="lvl-2"><p>Task-Adversarial Gradients</p></li><li class="lvl-2"><p>Domain-Adversarial Gradients</p></li></ul><p>When it comes to multisource DG where domain labels are provided, one<br>can exploit domain-adversarial gradients to synthesize domainagnostic<br>images.</p><ul class="lvl-0"><li class="lvl-2"><p>Random Augmentation Networks</p></li><li class="lvl-2"><p>Off-the-Shelf Style Transfer Models</p></li><li class="lvl-2"><p>Feature-Based Augmentation**</p></li></ul><h2 id="3-4-Ensemble-Learning">3.4 Ensemble Learning</h2><p>用不同的初始化参数多学几个模型，结果一平均</p><ul class="lvl-0"><li class="lvl-2"><p>Exemplar-SVMs</p></li><li class="lvl-2"><p>Domain-Specific Neural Networks</p></li></ul><p>A common practice is to learn domain-specific neural networks, each<br>specializing in a source domain [61], [198]. Rather than learning an<br>independent CNN for each source domain [198], it is more efficient,<br>and makes more sense as well, to share between source domains some<br>shallow layers [61], which capture generic features [139].</p><p>多学几个模型，共用头几层，然后训练</p><h3 id="Domain-Specific-Batch-Normalization">Domain-Specific Batch Normalization</h3><p>one for each source domain for collecting domain-specific statistics.<br>This is equivalent to constructing domain-specific classifiers but with<br>parameter sharing for most parts of a model except the normalization<br>layers.</p><p>共享大多数参数，但是在归一化层对不同的域做区隔</p><h3 id="Weight-Averaging">Weight Averaging</h3><p>aggregates model weights at different time steps during training to form<br>a single model at test time [239]. Unlike explicit ensemble learning<br>where multiple models (or model parts) need to be trained, weight<br>averaging is a more efficient solution as the model only needs to be<br>trained once. In [205], the authors have demonstrated that weight<br>averaging can greatly improve model robustness under domain shift. In<br>fact, such a technique is orthogonal to many other DG approaches and can<br>be applied as a postprocessing method to further boost the DG<br>performance.</p><h2 id="3-5-Self-Supervised-Learning？">3.5 Self-Supervised Learning？</h2><p>Recent state-of-the-art self-supervised learning methods [244],<br>[245] are mostly based on combining contrastive learning with data<br>augmentation. The key idea is to pull together the same instance (image)<br>undergone different transformations (e.g., random flip and color<br>distortion) while push away different instances to learn instanceaware<br>representations. Different from predicting transformations such as<br>rotation, contrastive learning aims to learn transformation-invariant<br>representations. Future work can explore whether invariances learned via<br>contrastive learning can better adapt to OOD data.</p><p>最近最好的自监督学习就是基于对抗学习和数据增强</p><h2 id="3-6-Learning-Disentangled-Representations">3.6 Learning Disentangled Representations</h2><p><em>This approach was later extended to neural networks in [37]. One can<br>also design domain-specific modules such as in [207] where<br>domain-specific binary masks are imposed on the final feature vector to<br>distinguish between domain-specific and domain-invariant components.<br>Another solution is to apply low-rank decomposition to a model’s weight<br>matrices in order to identify common features that are more<br>generalizable [208].</em></p><p>可以设计一个domain-specific的模块一个general模块，最后检测最后的特征层有哪些是general有哪些是specific的</p><h2 id="3-7-Regularization-Strategies">3.7 Regularization Strategies</h2><h2 id="3-8-Reinforcement-Learning">3.8 Reinforcement Learning</h2><h1>4 THEORIES</h1><h1>5 FUTURE RESEARCH DIRECTIONS</h1><h1>6 CONCLUSION</h1>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文摘录</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>8~9月读书摘抄</title>
    <link href="/posts/21143/"/>
    <url>/posts/21143/</url>
    
    <content type="html"><![CDATA[<h1>潜规则（推荐阅读原文，可读性高）</h1><h2 id="摘录">摘录</h2><p>  合法地祸害别人的能力，乃是官吏们的看家本领。这是一门真正的艺术，种种资源和财富正要据此分肥并重新调整。</p><p>  中国民间有句老话，叫作&quot;身怀利器，杀心自起&quot;。在如此实力悬殊的战争中，自己最多不过蹭破点皮，俘获的却是众多的子女玉帛，这样的仗自然就特别爱打，也特别能打。官吏们要顶住多打几仗的诱惑，很有必要定力过人。</p><p>  其实，中国历代老狼的经验很丰富，完全明白这个道理。那些为天子牧民或者叫牧羊的肉食者，都知道羊是狼生存的根本------简称&quot;民本&quot;。大家都懂得爱护羊群的重要意义。奈何抵抗不住眼前绵羊的诱惑，也抵抗不住生育狼崽子的诱惑。这也是有道理的：我不吃，别的狼照样吃；我不生，别的狼照样生。个体狼的利益与狼群的集体利益未必一致。如果我的节制不能导致别人的节制，我的自我约束对羊群来说就没有任何意义，徒然减少自己的份额而已。在老狼忍不住饕餮的时候，我可以听到一声叹息：它们要是变成刺猬，俺们不就变成清官了么？</p><p>  总之，从经济方面考虑，清官是很难当的。那时的正式制度惩罚清官，淘汰清官。硬要当清官的人，在经济上必定是一个失败者。当然，这里算的都是经济账，没有重视道德操守。道德操守是官僚集团自始至终卖力挥舞的一面大旗，它翻滚得如此夺目，根本就不容你不重视。我完全承认，道德的力量是有效的，海瑞的刚直不阿可以为证。但道德的力量又是有限的，海瑞的罕见和盛名也可以为证。</p><p>  恶政好比是一面筛子，淘汰清官，选择恶棍。</p><p>  在权力大小方面，皇上处于优势，官僚处于劣势。但是在信息方面，官吏集团却处于绝对优势。封锁和扭曲信息是他们在官场谋生的战略武器。你皇上圣明，执法如山，可是我们这里一切正常，甚至形势大好，你权力大又能怎么样？我们报喜不报忧。我们看着领导的脸色说话。说领导爱听的话。我们当面说好话，背后下毒手。难道有谁能天真地指望钱能向皇上汇报，说我最近成功地完成了两次敲诈勒索么？如果干坏事的收益很高，隐瞒坏事又很容易；如果做好事代价很高，而编一条好消息却容易，我们最后一定就会看到一幅现代民谣所描绘的图景：&quot;村骗乡，乡骗县，一级一级往上骗，一直骗到国务院。</p><p>  那么清官究竟在哪里呢？清官光荣地牺牲了，成了大家的好榜样。</p><p>  令人拍案叫绝的是，如今的官员和整个官场根本就用不着翻检什么古籍，他们无师自通，与明清官场患上了一模一样的病症，就连&quot;三节&quot;也和明清一样选在春节、端午和中秋，绕开了官定地位远高于端午的元旦、五一和国庆节。这真是莫名其妙，妙不可言。持续数十年的决裂传统和培育新人的凶狠努力，居然只造就了一点行贿名称和技巧上的差距。从发展速度判断，弥补这点差距，赶上并超过明清官场的水平，在不远的将来就可以实现，我对这一点要比&quot;超英赶美&quot;乐观得多。</p><p>  从利害关系的角度看，对抗当然是要倒霉的，听话才有出路，自己也可以跟着沾点光。但是从道德是非的角度看，欺下媚上毕竟有点不对劲。怎么办？这是每个官员都躲不开的实际问题，也是一个可以逼迫大多数人显现原形的问题。如果碰上思想不那么纯洁，立场不那么坚定的人，恐怕就会冒出这样的念头：我对抗领导，然后丢掉饭碗，真能起到什么好作用么？白白牺牲了自己，换上来一个新的，说不定一点良心也没有，欺压老百姓更加残酷，还不如我呢。为了减轻东阿人民的损失，我要坚守岗位，多跟领导合作，少搞对抗。------如此一想，良心竟然被我们糊弄平整了，我们也就可以坦然地媚上欺下了。这种官场生存策略的转变正好与晏氏转型相对应。</p><p>  皇上可以代表天道的高论，在清末民初就有很多人不信了。但是问题并没有彻底解决。民国号称是人民的国，偏偏不肯让人民当家作主，说要经过&quot;军政&quot;和&quot;训政&quot;这两个历史阶段之后，才能&quot;还政于民&quot;，实行宪政。这套说法的理论根据是三民主义。主义云云，听起来很像是天道的变种。中国共产党人揭露说，这套为&quot;还政于民&quot;设置条件的说法，本质是为一党独裁和蒋家王朝的专制打掩护。当然，这种揭露是很深刻的，但我觉得，宣称自己是老百姓的代理人，而且是临时代理人，总比宣称自己是天道的代表进了一步。毕竟他失去了可以永远领导老百姓的借口，失去了永远也不还政于民的理由。</p><p>  孔子和毛泽东倡导的人格理想，都很合时宜地告诉人们如何处理当时最基本的人际关系，并且很有力地证明这种处理方式合乎天道人心或者历史规律，具有终极价值。</p><p>  现在，我们离宗族大姓毗邻而居，宗亲世交触目皆是的时代已经很远了，我们离革命同志团结一心、兴无灭资、推翻三座大山的时代也不近了。出门上班，满眼小商小贩雇主雇员，下班上路，到处是行色匆匆的路人和讨价还价的顾客。想叫一声同志，招呼一声兄弟，真不知冲谁开口。</p><p>  孔夫子和毛主席没有教我们在这样的社会里怎样做人，没有来得及为我们树立一种合乎此时此境的人格理想，并把它与不知躲在何处的终极价值联系起来。</p><p>  呜呼，我们失去了精神支柱。</p><p>  我能拿出什么样的理由来劝阻呢？为人民服务？笑话。毫不利己专门利人？神经病。“五讲四美三热爱”、争当&quot;四有新人&quot;？大傻帽。天理良心，损阴德折阳寿，伤天害理，不得好死，断子绝孙，下十八层地狱，除了这些古人的话，我想不到什么更有力量的说法。我好像出了毛病，或者是我们现在的意识形态出了毛病，很容易就能找到替损人利己的行为辩护的理由，但是却找不到有力的反对理由。天理良心，这是宋明理学的东西，被几百年来的英雄好汉斥为假道学的东西。损阴德折阳寿下地狱，这是迷信，传媒们正在起劲地反对着。反对了，打倒了，然后呢？光天化日之下还剩下什么？</p><p>  所谓抬头三尺，即有神明，善有善报，恶有恶报，这些话在那时人们心目中的分量，绝对不同于现在的说笑。这种神秘的威胁是永远无法证实，也永远无法打消的，它永远是一把悬在贪官污吏头上的剑：欺压百姓不得好死。就算得了好死，地狱里也有油锅等着你。你可以不信，但是又不敢完全不信。“就是&quot;迷信&quot;的力量。我们可以说这是神道设教，说这是胡扯，但是你发明一个既不胡扯又有威慑力的说法试试？”</p><h2 id="感想">感想</h2><p>  这本书感觉已经把潜规则的来龙去脉说得很清楚了，无非既要把良心糊弄平整，又要穷奢极欲的情况下，老爷们官官相护地构架出了一套心照不宣的剥削体系。人们为何总是把自己的聪明才智挥霍在这种地方，处心积虑把伤害别人变得合法，合法的那么心安理得。</p><h1>是我把你蠢哭了嘛（不建议花时间看原书，可快速翻阅)</h1><h2 id="摘录（公正世界假说必看）">摘录（公正世界假说必看）</h2><h3 id="为什么批评比表扬效力更强">为什么批评比表扬效力更强</h3><p>  压力引起皮质醇分泌会对大脑产生直接影响会加强人的注意力，让记忆更加鲜明持久。而在我们受到批评时，分泌的皮质醇以及其他激素可能也会不同程度地导致相同情况的发生，让我们经历实实在在的身体反应、变得敏感，且相关记忆得到增强</p><p>  我们亲身经历负面事件时，会相应地产生种种情绪和感受，海马和杏仁核又开始活跃起来，最终在情绪上增强记忆，令记忆更为鲜明。</p><p>  获得赞扬之类的好事也会引起神经系统的反应，通过催产素（oxytocin）让我们感到愉悦，但是催产素起作用的方式没那么强烈，并且较为短暂。化学特性决定了催产素在大约五分钟内就会被排出血液循环；相反，皮质醇可以持续作用超过一小时，有时甚至长达两小时，其效应远比催产素更持久。</p><p>  什么事一旦成了规范，爱新奇的大脑就会倾向于通过习惯化过程将其滤除。因为既然一直发生，忽视它也安然无恙，为什么还要浪费宝贵的脑力资源来关注呢？（表扬和赞成是大多数的社会规范）</p><p>  轻度赞扬既成规范，批评就会变得更具冲击力，因为那是出格的。</p><p>  我们的大脑确实只会根据我们所知道的情况做出判断，而我们所知道的又都是基于自身的结论和经验，因此往往会基于自己的所作所为去评判他人的行为。那么，假如我们自己是出于社会规范的要求而表现得礼貌、说一些赞美之词，那其他人的行为是不是也都出于同样的考量呢？于是，我们获得的每一份赞美都显得有点可疑：它是不是真心的？而假如有人批评了你，那就不仅说明你做得糟糕，而且已经糟糕到让人宁愿破坏社会规范也要指出的程度。这下，批评的分量再次重过了赞扬。</p><h3 id="为什么蠢人有时候更自信">为什么蠢人有时候更自信</h3><p>  有人或许以为最后一定是最聪明的人把持着事情的进展方向，因为越聪明的人，完成的工作就越好。可我们常看到的实情却似乎是反直觉的，越是聪明的人，对自己的观点不太自信的可能性越高，越容易给人留下不那么自信的印象，因而不被信任的概率也就越高</p><p>  人们在意自己的社会地位和财富，而看上去比自己更聪明的人则可能构成一种威胁。那些体格更高大、更强壮的人虽然令人心惊胆战，但却是一种已知的属性。</p><p>  可是，一个比自己聪明的人就是个未知数了，他们的行为方式让你难以预料或者根本无法理解，大脑完全搞不清楚他们会不会带来危险。于是，&quot;万事小心好过事后后悔&quot;的古老本能便被激活，触发怀疑与敌意。当然一个人也可以通过学习钻研从而让自己变得更加聪明，但这远比改善体格更复杂，也更不确定。举重让你的胳膊强壮，而学习和聪明之间的联系就要松散得多。</p><h3 id="达宁-克鲁格效应">达宁-克鲁格效应</h3><p>  智力欠佳的人不仅在智力的能力上有所欠缺，而且在认识自己智力不足的能力上也有不足。再加上大脑的自我中心倾向也会掺和进来，对自己产生负面意见的可能性进一步受到抑制。况且，要认识到自己的局限和他人能力的优秀本来就是件需要智力的事情。于是我们就看到有的人在自己全无亲身经验的事情上自信满满地与他人激烈争论，哪怕对方已经在该问题上钻研了一辈子。我们的大脑只有自身经验可以借鉴，而我们的基本假设是人人都和自己一样。所以假如自己是个傻瓜的话，就会…</p><p>  聪明人&quot;对世界的感知可能也类似，但是表现为另一种形式。如果一个聪明人觉得某件事很简单，那么他们很可能会认为其他人也有同样的感觉。他们以自己的理解水平为标准，因而觉得自己的聪明程度属于普通</p><p>  此外，聪明的人普遍养成了学习新事物、获取新知识的习惯，因而更有可能认识到自己不是什么都懂，知道在各种领域都还有好多东西需要学习，于是他们在下结论、做声明时就不敢那么信誓旦旦。</p><h3 id="解释笑话如同解剖青蛙。有助于你理解，但青蛙死了">解释笑话如同解剖青蛙。有助于你理解，但青蛙死了</h3><h3 id="嘲讽和谩骂的来处">嘲讽和谩骂的来处</h3><p>  我们不仅意识到自己的集体身份，还知道自己在集体中所处的位置。</p><p>  这样一来，如果有谁做出集体不予认可的行为，那么既是对集体&quot;完整性&quot;的威胁，也提供了一个机会让其他成员可以踩着不称职者的肩提升自己的地位。于是，谩骂和嘲弄就来了。</p><h3 id="公正世界假说">公正世界假说</h3><p>  大脑有一种内在假定，认为世界是公平公正的，善有善报、恶有恶报。这种偏误有助于人们发挥社群作用，因为它有震慑和阻止恶行发生的意义，还让人愿意行善</p><p>  可惜，这种假定并不真实。恶行不一定受到惩罚，好人也常常遇到坏事。然而偏见扎根于大脑深处，让我们深信不疑。于是，当我们看到某个无辜的人遭遇可怕的不幸时，脑中就会出现不和谐音：世界是公平的，但发生在这个人身上的事情却不是。大脑可不喜欢不和谐，于是产生了两个选项：可以认为世界终究还是无情和随机的，也可以认定受害者肯定是做了什么坏事才罪有应得。后者虽然更无情，却让我们继续对世界抱有岁月静好的（错误）假定，因此我们会指责遭遇不幸的受害者。</p><p>  举例来说，当人们能够做些什么减轻受害者的痛苦，或是了解到受害者其后会获得赔偿时，对受害者的批评就比较少。而如果人们毫无办法帮助受害者，那么就会对其发起更严重的抨击。尽管看起来特别残酷，但与上述假说一脉相承的是，受害者如果没有光明的结局，那么他们必然罪有应得，难道不是吗？</p><p>  人们还更倾向于指责那些让自己产生强烈认同感的受害者。看见倒下的树砸中的是一个与自己年龄、种族、性别不同的人时，产生同情相对容易得多；而如果看到一个和自己年纪、身高体型、性别均相同的人，开着同样的车子撞上一栋和自家同样的房子时，指责对方愚蠢无能的可能性则大大提高，尽管毫无证据支持自己的这种反应。</p><p>  在前一个例子中，没有哪一点可以套用到我们自己头上，那就不妨怪罪事情的发生源自随机的巧合，毕竟影响不到我们。后一个例子却很容易联想到自己，所以大脑想要合理地将其解释为受害者的个人失误。必然是那个人自己的错，否则随机巧合不就也可能发生在我们身上了吗？单是想到这点就难受。</p><p>  可见，尽管我们的大脑有群居、友好的意愿，但它太在乎维护认同感和保持内心的平静了，若有什么人或事对此造成威胁，它宁愿让我们做出不公正的对待。</p><h3 id="对抑郁症的误解">对抑郁症的误解</h3><p>  大脑为了避免替受害者感到难过，会狠狠地扭转思维。而另外一种表现方式，就是给那些终结生命的人贴上&quot;自私&quot;的标签。</p><p>  如果自杀只因为那些人自我放纵或冷酷自私，那就是他们个人的问题了，不会波及我------这么一想，自我感觉就好多了。</p><h2 id="感想-2">感想</h2><p>  这本书把很多我脑海中已有但很模糊的的观念用科学的方式给出了可能的成因，其中最有价值的一点就是“世界公平假说”。它令我惊觉，有时候我会觉得有的人“自作自受”，原来可能只是我大脑想要合理地将某件事解释为受害者的个人失误，而不是同样的厄运也有可能降落在我身上。我也真的会特别在乎维护认同感和保持内心的平静……</p><h1>送你一颗子弹（推荐阅读原书，有趣）</h1><h2 id="摘录-2">摘录</h2><p>  再真诚的忧郁或者狂躁，也因为注视着你的眼睛，变成了一种表演，以至于你自己都忘记它是一种感受还是一种姿态</p><p>  人渴望被承认，也就是别人的目光，但是同时，当别人的目光围拢过来的时候，他又感到窒息，感不到自由</p><p>  集体生活中的&quot;强制性交往&quot;迫使你想独处的时候不得不面对他人，而孤魂野鬼的生活使你在想跟人说话的时候，不得不拿起电话，一个一个往下扫名字，并且自言自语：这个人有空吗？他呢？她呢？他？她？他？上次是我主动约他吃饭的，这次再约人家会不会觉得很烦？而且，其实我们好像也没有什么可说的？</p><p>  一切洗脑的成功要旨，不过在于帮助人们逃避自由。当一个体系能够用逻辑自洽的方式替你回答一切问题、并且保证这些答案的光荣伟大正确的时候，的确，为什么还要去承受&quot;肩负自由的疲惫&quot;呢？</p><p>  春天来的时候，总觉得会发生点什么，但是到头来，什么都没有发生，然后就觉得自己错过了点什么。</p><p>  大学的时候，一个朋友和男友分手，是他主动提出来的。她问为什么，他说：你所能想到的全部理由，都是对的</p><p>  自我是一个深渊，它如此庞大，爱情不可填补</p><p>  瞧瞧那帮男留学生，一个个长得丧权辱国的</p><p>  后来一个网友整理出了男人长相的几个档次：丧权辱国、闭关自守、韬光养晦、为国争光、精忠报国</p><p>  我毕生的理想，就是找个高高大大的男生，他就那么随便一帅，我就那么随便一赖，然后岁月流逝，我们磨磨蹭蹭地变老。</p><p>  不断有人跳出來说：'我们凭什么要关心政治？就爱吃喝玩乐自己挣钱自己花怎么了？！“当然，享乐主义是人权。谁也无权干预。但是千万不要以为&quot;政治冷演&quot;就是没有政治态度，冷漠就是你的态度。更重要的是，享乐主义得以存在，是有一系列公共制度的前提的，而这些前提是政治斗争的结果。以为私人生活与政治没有关系的人，忘记了私人领域从边界的界定到秩序的维护都是政诒问题，难道60年代全中国人穿灰黑蓝和今天大家穿得花枝招展仅仅是&quot;个人品位&quot;的不同？今天的中国从房价、到学术腐败、到电话费、到你在医院要排多久的队、到奴工、到孩子上学、到交通…哪一个归根结底不是&quot;政治向题”？那些&quot;我就是我，去他妈的政治&quot;的说法，就像是&quot;我就是我，去他妈的空气&quot;一样，貌似充满个性，其实无比滑稽。我理解在一个关心政治成本太高、追求个人发展动力很大的时代里大多数人的政治冷漠，但是我不能理解为这种冷漠而感到的洋洋得意。</p><p>  也许，他也只有通过&quot;不想&quot;来逃避这件事情的沉重，因为一个人认识论的飞跃恰恰就发生在&quot;想&quot;的那一刻，因为人道主义的起点在于&quot;一个&quot;人面对另&quot;一个&quot;受苦的人并且心里&quot;咯噔&quot;一声：如果我是他呢？</p><p>  讨论所有这些问题时,突然想到闹同学以前说到的一个玩笑。美国女人嫁人之后都要随丈夫姓，但是一个女权主义者抗议说：我不跟你姓，我要捍卫自己的权利，姓原来的姓！但是丈夫回答道;可是你原来的姓，是你父亲的姓，跟他姓本身，不过是父权制度的另一种表现形式而已！</p><p>  自由这个东西的神奇，不在于它会带来多少洪水猛兽，而在于，这些洪水猛兽出现以后，你发现它其实也不过如此</p><p>  别人往往记住了说话的语气，却忘记了这语气之下的信息。</p><h2 id="感想-3">感想</h2><p>  一个有趣的灵魂把自己的思考与感受用干脆利落的语言展示出来，有时候甚至还有些俏皮：）</p>]]></content>
    
    
    <categories>
      
      <category>读书</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>浅记录2022暑假</title>
    <link href="/posts/53677/"/>
    <url>/posts/53677/</url>
    
    <content type="html"><![CDATA[<h1>Part1 一次还不错的实习</h1><p>  在田间地头行走，被带刺的棘草轻轻抚摸。在雨后初霁的清晨，被湿润的泥土赠与了一双混凝土结构的鞋套。在“12点准点开餐”的饭厅，被先行一步的同学们悉心教会了残羹冷炙是为何物。</p><p>  而“世界以痛吻我，要我报之以歌”，泰戈尔如是说。</p><p>  乡间的风也便徐徐吹进我的心坎，停留片刻，又带着我明媚的好心晴吹进了田野，掀起阵阵稻浪。</p><p>  夏夜的星星也便明亮了起来，摘不下来，不如放一阵烟花，送几颗星星上天。</p><p>  写到这里，似乎这次实习让我印象深刻的东西似乎与遥感没什么关系。且留几个关键词吧，“无人机”“全站仪”“千寻”“通过多种技术路径实现遥感成图”。：）</p><h1>Part2 一阵还算认真的学习</h1><p>  该看的论文没看几行，单词查了一大堆，可惜没进脑子</p><p>  该敲的代码没敲几排，错误爆了一大串，还得谷哥帮我</p><p>  该搭的环境没搭几分钟，系统崩溃了好几次，没事从头再来</p><h1>Part3 一次令人摸不着头脑的隔离</h1><p>  码是绿的，人是低风险来的，隔离是马上的。什么，我朝发白帝的时候白帝还说自己是低风险，暮到江陵的时候,江陵说白帝它……</p><p>  那没事了</p><p>  载去隔离的悠悠路上，无意间注意到一位可能有着4、5个微信账号的小姐姐向着至少5、6个人传达着自己要隔离的消息（可能也许大概）。</p><p>  涨见识了。</p><p>  每天午后快乐的腾讯会议如期而至，开会和疏解心情两者放在一起不会八字不合嘛?真的为难防疫工作人员了</p><p>  不如算了</p><p>  隔离无聊不如搞点事情，搭了一个个人博客。没错，就是你正在看的这个。</p><p>  假结束了</p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周期总结</tag>
      
      <tag>季度总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>简单记录一下本博客的搭建过程</title>
    <link href="/posts/57019/"/>
    <url>/posts/57019/</url>
    
    <content type="html"><![CDATA[<h1>搭骨架</h1><ol><li class="lvl-3">在官网上下载node.js， 在本步骤中会下载好node和npm两个工具， 在命令行中使用npm -v来查验</li><li class="lvl-3">使用命令行输入<code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code>来安装cnpm，并使用了淘宝镜像源加速</li><li class="lvl-3">命令行输入<code>cnpm install -g hexo-cli</code>安装hexo</li><li class="lvl-3">找一个地方创建存放blog的文件夹，之后的操作若无说明均在blog文件层操作</li><li class="lvl-3">命令行输入<code>hexo init</code>初始化获得博客基本框架，这个时候使用<code>hexo s</code>就能看到本地接口的博客界面啦</li></ol><h1>代码托管</h1><ol><li class="lvl-3"><p>在git中创建仓库，仓库名一定要设置为git的用户名</p></li><li class="lvl-3"><p>在文件中找到_config.yml, 在文件的最末尾处修改和添加:</p></li></ol><p>···</p><pre><code class="hljs">type: gitrepository: 代码的url或sshbranch: master</code></pre><p>···</p><ol start="3"><li class="lvl-3"><p>安装提交代码的插件<code>cnmp install --save hexo-deployer-git</code></p></li><li class="lvl-3"><p>命令行<code>hexo d</code>推送本地文件到仓库，然后就可以用&quot;<a href="http://xn--eqr924avxo.github.io">用户名.github.io</a>&quot;远程访问博客了</p></li></ol><h1>换博客主题</h1><ol><li class="lvl-3"><p>查找论坛或者hexo官网找到合眼缘的主题找到对应的git库</p></li><li class="lvl-3"><p>git clone该库到blog/themes中</p></li><li class="lvl-3"><p>在_config.yml中的theme处修改为你要用的库</p></li><li class="lvl-3"><p>重新推送你的库到git中就好了</p></li><li class="lvl-3"><p>具体页面的配置参考你的主题作者提供的参考</p></li></ol><h1>一些简单的使用</h1><ol><li class="lvl-3"><p><code>hexo clean</code>清除已有的文件</p></li><li class="lvl-3"><p><code>hexo g</code>重新生成</p></li><li class="lvl-3"><p><code>hexo s</code>在本地预览</p></li><li class="lvl-3"><p><code>hexo d</code>推送到git库</p></li></ol><p>注：前三个为一套，可以每次修改配置的时候进行操作。如果只是写好单篇博客上传则只用4即可</p><h1>一些注意事项</h1><ol><li class="lvl-3"><p>建议使用ssh作连接，在git改变安全机制之后url不大好用，至少本人找不到好的解决方案应用到博客的创建</p></li><li class="lvl-3"><p>建议找主题的时候找参考写的比较详细，适合和我一样的入门者做一些基础的设置</p></li></ol><h1>引用</h1><ol><li class="lvl-3"><p><a href="https://www.bilibili.com/video/BV1Yb411a7ty?spm_id_from=333.880.my_history.page.click">可以全部follow的b站视频</a></p></li><li class="lvl-3"><p><a href="https://github.com/fluid-dev/hexo-theme-fluid">我使用的主题</a></p></li></ol><h1>后续的一些补充</h1><ol><li class="lvl-3"><p>不建议在管理员权限下创建博客，直接正常的权限创建即可。若在管理员权限下则不利于在其它的编辑器下直接修改文件</p></li><li class="lvl-3"><p>建议更换hexo内置的markdown渲染引擎，方法参考<a href="http://t.csdn.cn/QayjR">文章</a>。原始的引擎有些markdown语法无法正常渲染成字符</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>杂</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
